{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taladesuru/UH-English-Premier-League-Football-Prediction-Project-/blob/main/English%20premier%20League%20prediction%3A%20The%20Analysis%20of%20The%20Relevance%20of%20Football%20Teams%E2%80%99%20Home%20Advantage%20Point%20AccumulationV5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive To Access Dataset"
      ],
      "metadata": {
        "id": "s7DtuZ_UWZGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D-x--AG0bga5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Library Importation"
      ],
      "metadata": {
        "id": "jCVO8KnjWpN3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBDeb6NnbUVS"
      },
      "outputs": [],
      "source": [
        "#Import the Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read The 2023-2024 premier league Dataset csv file into the pl2324 DataFrame\n",
        "pl2324 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/2023-2024.csv')"
      ],
      "metadata": {
        "id": "knpyO7a4cDCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display The top  5 and bottom 5 rows of the pl2324 dataframe\n",
        "pl2324"
      ],
      "metadata": {
        "id": "4GiD7m0rk5_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display The Summary of Information of the Dataset File\n",
        "pl2324.info()"
      ],
      "metadata": {
        "id": "-Adhslssk8J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA PREPROCESSING**"
      ],
      "metadata": {
        "id": "_HDB8a8Fl_KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA CLEANING"
      ],
      "metadata": {
        "id": "a_UxK9YdlZ7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and count for missing values\n",
        "pl2324.isnull().sum()"
      ],
      "metadata": {
        "id": "bRALq5Sjl18_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Date column to datetime format with the specified date format\n",
        "pl2324['Date'] = pd.to_datetime(pl2324['Date'], format='%d/%m/%Y')"
      ],
      "metadata": {
        "id": "TQJNRo2Ql4ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the datatype of the columns of the pl2324 DataFrame\n",
        "pl2324.dtypes"
      ],
      "metadata": {
        "id": "kvoe3F--hF-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with unique team names\n",
        "teams = pl2324['HomeTeam'].unique()\n",
        "pl2324_table = pd.DataFrame({'Team': teams})\n",
        "\n",
        "# Display the DataFrame\n",
        "pl2324_table"
      ],
      "metadata": {
        "id": "4Zz63he-Ivml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Dataframe"
      ],
      "metadata": {
        "id": "qaG_jDX1jixn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize relevant columns in the dataframe and the default value created is set to zero\n",
        "pl2324_table[['Played','Win','Draw','Loss','GF','GA','GD','Points']] = 0\n",
        "\n",
        "# Check Table\n",
        "pl2324_table"
      ],
      "metadata": {
        "id": "DHnAgRqAJ_bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index of pl2324_table to make 'Team' a regular column\n",
        "pl2324_table = pl2324_table.reset_index()\n",
        "\n",
        "# Set 'Team' as the new index\n",
        "pl2324_table = pl2324_table.set_index('Team')\n",
        "\n",
        "# Iterate over each row in the DataFrame pl2324\n",
        "for i, row in pl2324.iterrows():\n",
        "    home = row['HomeTeam']\n",
        "    away = row['AwayTeam']\n",
        "\n",
        "    # Update 'Played' count for home and away teams\n",
        "    pl2324_table.loc[home, 'Played'] += 1\n",
        "    pl2324_table.loc[away, 'Played'] += 1\n",
        "\n",
        "    # Update 'GF' (Goals For) and 'GA' (Goals Against) for home and away teams\n",
        "    pl2324_table.loc[home, 'GF'] += row['FTHG']\n",
        "    pl2324_table.loc[away, 'GF'] += row['FTAG']\n",
        "    pl2324_table.loc[home, 'GA'] += row['FTAG']\n",
        "    pl2324_table.loc[away, 'GA'] += row['FTHG']\n",
        "\n",
        "    # Update 'Win', 'Loss', 'Draw' counts based on match result\n",
        "    if row['FTR'] == \"H\":\n",
        "        pl2324_table.loc[home, 'Win'] += 1\n",
        "        pl2324_table.loc[away, 'Loss'] += 1\n",
        "    elif row['FTR'] == \"A\":\n",
        "        pl2324_table.loc[away, 'Win'] += 1\n",
        "        pl2324_table.loc[home, 'Loss'] += 1\n",
        "    else:\n",
        "        pl2324_table.loc[home, 'Draw'] += 1\n",
        "        pl2324_table.loc[away, 'Draw'] += 1\n",
        "\n",
        "# Calculate 'Points' as 3 times 'Win' plus 'Draw'\n",
        "pl2324_table['Points'] = 3 * pl2324_table['Win'] + pl2324_table['Draw']\n",
        "\n",
        "# Calculate 'GD' (Goal Difference) as 'GF' minus 'GA'\n",
        "pl2324_table['GD'] = pl2324_table['GF'] - pl2324_table['GA']\n",
        "\n",
        "# Sort teams by 'Points' in descending order\n",
        "pl2324_table = pl2324_table.sort_values(by='Points', ascending=False)\n",
        "\n",
        "# Reset the index to have a sequential index\n",
        "pl2324_table = pl2324_table.reset_index()\n",
        "\n",
        "pl2324_table"
      ],
      "metadata": {
        "id": "4UOF0PIvKhis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yoSzwEYtmvkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above updates the standings table pl2324_table for the league based on match results from the DataFrame pl2324, which calculates the number of games played, goals for (GF), goals against (GA), wins, losses, draws, points, and goal difference (GD) for each team.\n",
        "\n",
        "                         **Furthermore:**\n",
        "\n",
        "1. **Reset and Set Index:** The pl2324_table index is reset to make Team a regular column, then the Team is set as the index.\n",
        "\n",
        "2. **Iterate Over Matches:** For each match in pl2324, the home and away teams are identified. The number of games played (Played), goals for (GF), goals against (GA), wins (Win), losses (Loss), and draws (Draw) are updated based on the match full time result (FTR).\n",
        "\n",
        "3. **Calculate Points and Goal Difference:** The Point is calculated as 3 points for each win plus 1 point for each draw, while the goal difference (GD) is calculated as goals for (GF) minus goals against (GA).\n",
        "\n",
        "4. **Sort and Reset Index:** Teams are sorted by Points in descending order, while the index is reset to ensure a sequential index."
      ],
      "metadata": {
        "id": "wldrMkUyjl0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(pl2324_table.Team)))\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Plot bars with unique colors\n",
        "for i, league_team in enumerate(pl2324_table.Team):\n",
        "    plt.bar(league_team, pl2324_table.Win[i], color=colors[i], label=league_team)\n",
        "\n",
        "plt.xlabel('league_team')\n",
        "plt.ylabel('Wins')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Total Premier League Wins 2023-2024')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fJ9XqR53R8oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(pl2324_table.Team)))\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Plot bars with unique colors\n",
        "for i, league_team in enumerate(pl2324_table.Team):\n",
        "    plt.bar(league_team, pl2324_table.Loss[i], color=colors[i], label=league_team)\n",
        "\n",
        "plt.xlabel('league_team')\n",
        "plt.ylabel('Loss')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Total Premier League Losses 2023-2024')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3PBzl4rMWqxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(pl2324_table.Team)))\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Plot bars with unique colors\n",
        "for i, league_team in enumerate(pl2324_table.Team):\n",
        "    plt.bar(league_team, pl2324_table.Draw[i], color=colors[i], label=league_team)\n",
        "\n",
        "plt.xlabel('league_team')\n",
        "plt.ylabel('Draw')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Total Premier League Draw 2023-2024')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), ncol=1, fontsize='small')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Gpup4b5XLe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324.head()"
      ],
      "metadata": {
        "id": "y3acklTPr1-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Home Game Total Counts\n",
        "pl2324.HomeTeam.value_counts()"
      ],
      "metadata": {
        "id": "B0YEtxKTr3Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Away Game Total Count\n",
        "pl2324.AwayTeam.value_counts()"
      ],
      "metadata": {
        "id": "mZU6dT3r1Edg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Missing Values\n",
        "missing_val = pl2324.isnull().sum()\n",
        "\n",
        "# Display Columns with Missing Values\n",
        "missing_columns = missing_val[missing_val > 0]\n",
        "missing_columns"
      ],
      "metadata": {
        "id": "Lun_gQUX1MVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following has missing values in the in the columns inside the dataset:\n",
        "\n",
        "BWH, BWD, BWA: 2 missing values each\n",
        "\n",
        "IWH, IWD, IWA: 182 missing values each\n",
        "\n",
        "P>2.5, P<2.5: 8 missing values each\n",
        "\n",
        "BWCH, BWCD, BWCA: 12 missing values each\n",
        "\n",
        "IWCH, IWCD, IWCA: 182 missing values each\n",
        "\n",
        "PC>2.5, PC<2.5: 7 missing values each\n",
        "\n",
        "MaxCAHH, MaxCAHA: 1 missing value each"
      ],
      "metadata": {
        "id": "n05njb1fa0jL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values for columns with a small number of missing values using the median statistics\n",
        "for col in ['BWH', 'BWD', 'BWA', 'P>2.5', 'P<2.5', 'BWCH', 'BWCD', 'BWCA', 'PC>2.5', 'PC<2.5', 'MaxCAHH', 'MaxCAHA']:\n",
        "    pl2324[col].fillna(pl2324[col].median(), inplace=True)\n",
        "\n",
        "# Drop columns with a large number of missing values\n",
        "pl2324.drop(columns=['IWH', 'IWD', 'IWA', 'IWCH', 'IWCD', 'IWCA'], inplace=True)\n",
        "\n",
        "# Select relevant features for analysis\n",
        "select_columns = ['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG'] + \\\n",
        "                   [col for col in pl2324.columns if 'B365' in col or 'Max' in col or 'Avg' in col]\n",
        "\n",
        "select_data = pl2324[select_columns]\n",
        "\n",
        "# Display the first few rows of the preprocessed dataset\n",
        "select_data\n"
      ],
      "metadata": {
        "id": "MD1qBWvyaL5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLORATORY DATA ANALYSIS"
      ],
      "metadata": {
        "id": "ygxLxZOEi83j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats"
      ],
      "metadata": {
        "id": "2hdDkx2VpRSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the Data Statistics for pl2324\n",
        "\n",
        "# Display results\n",
        "print(\"Data Statistics:\\n\", data_stats)\n"
      ],
      "metadata": {
        "id": "BWx9jGWTfjNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the Data Statistics for pl2324_table\n",
        "data_stats = pl2324_table.describe()\n",
        "\n",
        "# Calculate mode\n",
        "mode_values = pl2324_table.mode().iloc[0]\n",
        "\n",
        "# Display results\n",
        "print(\"Data Statistics:\\n\", data_stats)\n",
        "print(\"\\nMode Values:\\n\", mode_values)"
      ],
      "metadata": {
        "id": "Cxszz88qE-Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate goals scored at Home by each Team\n",
        "home_goals_by_team = pl2324.groupby('HomeTeam')['FTHG'].sum().reset_index()\n",
        "\n",
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(home_goals_by_team)))\n",
        "\n",
        "# Create a bar chart with unique colors\n",
        "plt.figure(figsize=(15, 7))\n",
        "for i, team in enumerate(home_goals_by_team['HomeTeam']):\n",
        "    plt.bar(team, home_goals_by_team['FTHG'][i], color=colors[i], label=team)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Total Home Goals Scored by Each Team')\n",
        "plt.xlabel('Home Team')\n",
        "plt.ylabel('Total Home Goals')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XREGZjAGcSMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate goals scored at home by each team\n",
        "home_goals_by_team = pl2324_table.groupby('Team')[['GF', 'GA']].sum().reset_index()\n",
        "\n",
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(home_goals_by_team)))\n",
        "\n",
        "# Create a bar chart with unique colors\n",
        "plt.figure(figsize=(15, 7))\n",
        "for i, row in home_goals_by_team.iterrows():  # Iterate over rows using iterrows()\n",
        "    team = row['Team']\n",
        "    goals_for = row['GF']\n",
        "    goals_against = row['GA']\n",
        "    plt.bar(team, [goals_for, goals_against], color=colors[i], label=team)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Total Home Goals Scored by Each Team')\n",
        "plt.xlabel('Home Team')\n",
        "plt.ylabel('Total Home Goals')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lb3PHGnbF9bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(pl2324_table)))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot histogram for each team\n",
        "for i, team in enumerate(pl2324_table['Team']):\n",
        "    team_data = pl2324_table[pl2324_table['Team'] == team]['GF']\n",
        "    sns.histplot(team_data, bins=5, kde=False, color=colors[i], label=team, alpha=0.7)  # Adjust bins as needed\n",
        "\n",
        "plt.title('Histogram of Full Time Home Goals by Team 2023 - 2024 Season')\n",
        "plt.xlabel('Full Time Home Goals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(False)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OytiFASxHdYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(pl2324_table['Team'].unique())))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot histogram for each team\n",
        "for i, team in enumerate(pl2324_table['Team'].unique()):\n",
        "    team_data = pl2324_table[pl2324_table['Team'] == team]['GF']\n",
        "    sns.histplot(team_data, bins=5, kde=False, color=colors[i], label=team, alpha=0.7)\n",
        "\n",
        "plt.title('Histogram of Full Time Home Goals by Team 2023 - 2024 Season')\n",
        "plt.xlabel('Full Time Home Goals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(False)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JZdO4ZgoJ5QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Bar chart to Visualize the Full Time Result (FTR)\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.countplot(pl2324['FTR'], palette='Set2')\n",
        "plt.title('Bar Chart of Count of Full Time Result for 2023 - 2024 Season')\n",
        "plt.xlabel('Full Time Result for total count of HOME_WIN, AWAY_WIN and DRAW')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4dCC1WJqlbs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Line graph to Visualize the Average Betting Odds for Home Win (AvgCAHH) over Time\n",
        "#plt.figure(figsize=(12, 6))\n",
        "#pl2324['Date'] = pd.to_datetime(pl2324['Date'])\n",
        "#pl2324.sort_values(by='Date', inplace=True)\n",
        "#plt.plot(pl2324['Date'], pl2324['AvgCAHH'], marker='o', color='darkred')\n",
        "#plt.title('Average Betting Odds for Home Win Over Time')\n",
        "#plt.xlabel('Date')\n",
        "#plt.ylabel('Average Betting Odds for Home Win')\n",
        "#plt.xticks(rotation=45)\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "DgPM4NuwmUOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table.head()"
      ],
      "metadata": {
        "id": "oegac2I12TAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table.describe()"
      ],
      "metadata": {
        "id": "MOaI7bQnq6XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = pl2324_table[['Played', 'Win', 'Draw', 'Loss', 'GF', 'GA', 'GD', 'Points']].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "correlation_matrix"
      ],
      "metadata": {
        "id": "iDUQI0pqoAEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Correlations:**\n",
        "\n",
        "Goals For (GF) and Points: A strong positive correlation (0.91) means teams that scored more goals earned more points. This is because the more goals scored, the higher it is essential for winning.\n",
        "\n",
        "Goals For (GF) and Wins: A high correlation (0.91) signified that the teams that scored more goals were likely to win more matches.\n",
        "\n",
        "Points and Wins: A very strong correlation (1) suggests that teams with more points also had more wins.\n",
        "\n",
        "Points and Goal Difference (GD): A strong correlation (0.97) indicated that teams with a higher goal difference earned more points.\n",
        "\n",
        "**Negative Correlations:**\n",
        "\n",
        "Loss and Points: A strong negative correlation (-0.99) showed that teams with more losses had fewer points. This was expected as losses meant fewer points were earned.\n",
        "\n",
        "Loss and Goals For (GF): The negative correlation (-0.97) suggests that teams that lost more matches scored fewer goals.\n",
        "\n",
        "Goal Difference (GD) and Losses: The negative correlation (-0.92) meant teams with larger goal difference are less likely to suffer losses.\n",
        "Key Observations:\n",
        "\n",
        "Goals are crucial: The strong correlations between goals scored (GF) and wins, points, and goal difference highlighted the significant role goals played in the teams success.\n",
        "\n",
        "Losses are detrimental: The negative correlations between losses and points, goals scored, and goal difference underscore the importance of minimizing losses.\n",
        "\n",
        "In summary: This correlation matrix provides a clear picture of the strong relationships between the premier league team's goals, wins, losses, goal difference, and ultimately, their points earned. It gives emphasis to the importance of scoring goals and minimizing losses to achieve success."
      ],
      "metadata": {
        "id": "xvWgYwnmjiO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique colors for each team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Pairplot with unique colors for each team\n",
        "sns.pairplot(pl2324_table[['Team', 'Win', 'Draw', 'Loss', 'Points']], hue='Team', palette=palette)\n",
        "plt.suptitle('Pairplot of Selected Features', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Pairplot to visualize relationships\n",
        "#sns.pairplot(pl2324[['FTHG', 'FTAG', 'AvgCAHH', 'AvgCAHA']], palette='Set2')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "9fHAe9kWsofm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Plots:\n",
        "\n",
        "Diagonal Plots (Histograms): The diagonal subplots represent the distribution of each variable (e.g., FTHG, FTAG, AvgCAHH, AvgCAHA). Each histogram shows the frequency distribution of the respective variable, giving insights into its central tendency and spread.\n",
        "\n",
        "Off-Diagonal Plots (Scatter Plots): The off-diagonal subplots are scatter plots showing the relationship between pairs of variables. Each scatter plot helps identify correlations or patterns between two variables.\n",
        "\n",
        "For example, AvgCAHH vs. AvgCAHA shows a strong negative correlation, indicating that as the average betting odds for a home win (AvgCAHH) decrease, the average betting odds for an away win (AvgCAHA) increase."
      ],
      "metadata": {
        "id": "88QB323TOr2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "Cmcctc-j10ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE HOME GAME FEATURES"
      ],
      "metadata": {
        "id": "mbaA1boS1TeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create new features for home games\n",
        "# pl2324['HomeWin'] = np.where(pl2324['FTR'] == 'H', 1, 0)\n",
        "# pl2324['HomeDraw'] = np.where(pl2324['FTR'] == 'D', 1, 0)\n",
        "# pl2324['HomeLoss'] = np.where(pl2324['FTR'] == 'A', 1, 0)\n",
        "# pl2324['HomePoints'] = np.where(pl2324['FTR'] == 'H', 3, np.where(pl2324['FTR'] == 'D', 1, 0))\n",
        "\n",
        "# # Aggregate features for each team\n",
        "# team_stats = pl2324.groupby('HomeTeam').agg({\n",
        "#     'HomeWin': 'sum',\n",
        "#     'HomeDraw': 'sum',\n",
        "#     'HomeLoss': 'sum',\n",
        "#     'HomePoints': 'sum'\n",
        "# }).reset_index()\n",
        "\n",
        "# print(team_stats)"
      ],
      "metadata": {
        "id": "f0vAs98-15BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE AWAY GAME FEATURES"
      ],
      "metadata": {
        "id": "5bkXaQFe1eAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create new features for away games\n",
        "# pl2324['AwayWin'] = np.where(pl2324['FTR'] == 'A', 1, 0)\n",
        "# pl2324['AwayDraw'] = np.where(pl2324['FTR'] == 'D', 1, 0)\n",
        "# pl2324['AwayLoss'] = np.where(pl2324['FTR'] == 'H', 1, 0)\n",
        "# pl2324['AwayPoints'] = np.where(pl2324['FTR'] == 'A', 3, np.where(pl2324['FTR'] == 'D', 1, 0))\n",
        "\n",
        "# # Aggregate features for each team\n",
        "# team_stats_away = pl2324.groupby('AwayTeam').agg({\n",
        "#     'AwayWin': 'sum',\n",
        "#     'AwayDraw': 'sum',\n",
        "#     'AwayLoss': 'sum',\n",
        "#     'AwayPoints': 'sum'\n",
        "# }).reset_index()\n",
        "\n",
        "# print(team_stats_away)"
      ],
      "metadata": {
        "id": "c31nfLog4lnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Home Wins and Points"
      ],
      "metadata": {
        "id": "ADZp9Hec4X0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Visualize Home Wins and Points\n",
        "# plt.figure(figsize=(12, 6))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# sns.barplot(x='HomeTeam', y='HomeWin', data=team_stats)\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.title('Number of Home Wins by Team')\n",
        "# print('\\n')\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# sns.barplot(x='HomeTeam', y='HomePoints', data=team_stats)\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.title('Home Game Points by Team')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "fQCWWLeo9Qqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze if teams with more Home game points do better in the overall league standing at the end of the season (38 games)\n"
      ],
      "metadata": {
        "id": "eWWkyUcXFpFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new features for Home Game Points\n",
        "pl2324['HomePoints'] = np.where(pl2324['FTR'] == 'H', 3, np.where(pl2324['FTR'] == 'D', 1, 0))\n",
        "\n",
        "# Aggregate Home Game Points for each team\n",
        "home_points = pl2324.groupby('HomeTeam').agg({'HomePoints': 'sum'}).reset_index()\n",
        "home_points.rename(columns={'HomeTeam': 'Team'}, inplace=True)\n",
        "\n",
        "# Put the HomePoint in an Descending Order\n",
        "home_points = home_points.sort_values(by='HomePoints', ascending=False)\n",
        "\n",
        "print(home_points)"
      ],
      "metadata": {
        "id": "Xhghc7uQESkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by HomePoints in descending order to determine FinalStanding\n",
        "home_points = home_points.sort_values(by='HomePoints', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Assign FinalStanding based on sorted order\n",
        "home_points['HomeGameFinalStanding'] = home_points.index + 1\n",
        "\n",
        "# Display the adjusted standings\n",
        "print(home_points)"
      ],
      "metadata": {
        "id": "dfWSdjrP7QYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z32CsxjmJX7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot to visualize the relationship\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='HomePoints', y='HomeGameFinalStanding', data=home_points, hue='Team', palette='tab20', s=100)\n",
        "plt.xlabel('Home Game Points')\n",
        "plt.ylabel('Final Standing (Lower is Better)')\n",
        "plt.title('Home Game Points vs Home Game Final Standing')\n",
        "plt.gca().invert_yaxis() # Invert the y-axis to show lower standing as better\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "correlation = home_points['HomePoints'].corr(home_points['HomeGameFinalStanding'])\n",
        "print('\\n')\n",
        "print(f'Correlation between Home Game Points and Home Game Final Standing: {correlation}')\n"
      ],
      "metadata": {
        "id": "GJ5c5wYGAryK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding features to Build and Analyze our Model\n",
        "pl2324_table[['shots','shots_on_targets','shots_against','shots_on_targets_against']] = 0\n",
        "\n",
        "pl2324_table.head()"
      ],
      "metadata": {
        "id": "93hsiswGRTGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding New Features to the pl2324_TABLE"
      ],
      "metadata": {
        "id": "2JvmUl7dRuDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding features to Build and Analyze our Model\n",
        "pl2324_table[['shots','shots_on_targets','shots_against','shots_on_targets_against']] = 0\n",
        "\n",
        "# Set the 'Team' column as the index BEFORE the loop\n",
        "pl2324_table = pl2324_table.set_index('Team')\n",
        "\n",
        "pl2324_table.head()\n",
        "\n",
        "for i in pl2324.index:\n",
        "    home = pl2324.HomeTeam.loc[i]\n",
        "    away = pl2324.AwayTeam.loc[i]\n",
        "    pl2324_table.loc[home,'shots'] += pl2324.HS.loc[i]\n",
        "    pl2324_table.loc[home,'shots_on_targets'] += pl2324.HST.loc[i]\n",
        "    pl2324_table.loc[away,'shots'] += pl2324.AS.loc[i]\n",
        "    pl2324_table.loc[away,'shots_on_targets'] += pl2324.AST.loc[i]\n",
        "    pl2324_table.loc[home,'shots_against'] += pl2324.AS.loc[i]\n",
        "    pl2324_table.loc[home,'shots_on_targets_against'] += pl2324.AST.loc[i]\n",
        "    pl2324_table.loc[away,'shots_against'] += pl2324.HS.loc[i]\n",
        "    pl2324_table.loc[away,'shots_on_targets_against'] += pl2324.HST.loc[i]\n",
        "pl2324_table = pl2324_table.reset_index()"
      ],
      "metadata": {
        "id": "ABM96PqdRr_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table"
      ],
      "metadata": {
        "id": "OqYoOxtPSAf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Stages to get the Performance Rate for each Team**\n",
        "\n",
        "(A) Goal Difference (GD)"
      ],
      "metadata": {
        "id": "-8mDb5FySTqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Goal Difference (Goals Scored - Goals Conceded)\n",
        "pl2324_table['GD'] = pl2324_table['GF'] - pl2324_table['GA']\n",
        "\n",
        "# Sort by Goal Difference\n",
        "pl2324_table = pl2324_table.sort_values(by='GD', ascending=False)\n",
        "\n",
        "print(pl2324_table[['Team', 'GD']])\n",
        "print('\\n')\n",
        "\n",
        "# Getting the unique color for each Team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Plot a Barchart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Team', y='GD', data=pl2324_table, palette=palette)  # Apply the custom palette here\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Team')\n",
        "plt.ylabel('Goal Difference')\n",
        "plt.title('Goal Difference by Team')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NW32br_7Saxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(B) **Points Per Game (PPG)**: This normalizes performance across the numerous numbers of matches played."
      ],
      "metadata": {
        "id": "85shzr7vTIMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Points Per Game\n",
        "pl2324_table['PPG'] = pl2324_table['Points'] / pl2324_table['Played']\n",
        "\n",
        "# Sort by Points Per Game\n",
        "pl2324_table = pl2324_table.sort_values(by='PPG', ascending=False)\n",
        "\n",
        "print(pl2324_table[['Team', 'PPG']])\n",
        "print('\\n')\n",
        "\n",
        "# Getting the unique color for each Team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Plot a Barchart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Team', y='PPG', data=pl2324_table, palette=palette)  # Apply the custom palette here\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Team')\n",
        "plt.ylabel('Points Per Game')\n",
        "plt.title('Points Per Game by Team')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZPy_NiSATguK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C) Shot Conversion Rate (SCR): This measures how efficiently a team turns shots into goals"
      ],
      "metadata": {
        "id": "CpkDmvd4U6BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Shot Conversion Rate (Goals Scored / Total Shots)\n",
        "pl2324_table['SCR'] = pl2324_table['GF'] / pl2324_table['shots']\n",
        "\n",
        "# Handle potential division by zero (teams with no shots)\n",
        "pl2324_table['SCR'] = pl2324_table['SCR'].fillna(0)\n",
        "\n",
        "# Sort by Shot Conversion Rate\n",
        "pl2324_table = pl2324_table.sort_values(by='SCR', ascending=False)\n",
        "\n",
        "print(pl2324_table[['Team', 'SCR']])\n",
        "print('\\n')\n",
        "\n",
        "# Getting the unique color for each Team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Plot a Barchart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Team', y='SCR', data=pl2324_table, palette=palette)  # Apply the custom palette here\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Team')\n",
        "plt.ylabel('Short Conversion Rate')\n",
        "plt.title('Short Conversion Rate by Team')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6MJXXaOMU5HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(D) Weighing the Performance Index of each Team"
      ],
      "metadata": {
        "id": "6rbF37LKViur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weights (adjust these based on your priorities)\n",
        "weight_GD = 0.5\n",
        "weight_PPG = 0.3\n",
        "weight_SCR = 0.2\n",
        "\n",
        "# Calculate a weighted performance index\n",
        "pl2324_table['PerformanceIndex'] = (weight_GD * pl2324_table['GD']) + \\\n",
        "                               (weight_PPG * pl2324_table['PPG']) + \\\n",
        "                               (weight_SCR * pl2324_table['SCR'])\n",
        "\n",
        "# Sort by Performance Index\n",
        "pl2324_table = pl2324_table.sort_values(by='PerformanceIndex', ascending=False)\n",
        "\n",
        "print(pl2324_table[['Team', 'PerformanceIndex']])\n",
        "print('\\n')\n",
        "\n",
        "# Getting the unique color for each Team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Plot a Barchart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Team', y='PerformanceIndex', data=pl2324_table, palette=palette)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Team')\n",
        "plt.ylabel('Performance Index')\n",
        "plt.title('Performance Index by Team')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ue-l9xw9Vjzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table\n"
      ],
      "metadata": {
        "id": "r4Apuce4WpOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILDING MODEL and ANALYSIS"
      ],
      "metadata": {
        "id": "mmNyuhYm96JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "_BpBWTruspnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting Relevant Features and Target to use"
      ],
      "metadata": {
        "id": "o0POq2TYJ4pj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SI161XcdanKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table.columns.unique()"
      ],
      "metadata": {
        "id": "xARMoBspmPsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the unique columns of the dataset\n",
        "pl2324.columns.unique()"
      ],
      "metadata": {
        "id": "pHXDQ9wOKJUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features and target\n",
        "features_table = ['Played', 'Win', 'Draw', 'Loss', 'GF', 'GA', 'GD', 'Points',\n",
        "                 'shots', 'shots_on_targets', 'shots_against', 'shots_on_targets_against',\n",
        "                 'PPG', 'SCR', 'PerformanceIndex']\n",
        "\n",
        "features_pl2324 = ['HTHG', 'HTAG', 'FTHG', 'FTAG', 'HS', 'AS', 'HST', 'AST']\n",
        "target = 'FTR'\n",
        "\n",
        "# Extract features and target variable\n",
        "X_table = pl2324_table[features_table]\n",
        "X_pl2324 = pl2324[features_pl2324]\n",
        "y = pl2324['FTR']\n",
        "\n",
        "# One-Hot Encoding for 'HomeTeam' and 'AwayTeam'\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "encoded_teams = encoder.fit_transform(pl2324[['HomeTeam', 'AwayTeam']]).toarray()\n",
        "\n",
        "# Convert the NumPy array to a DataFrame\n",
        "X_encoded_teams = pd.DataFrame(encoded_teams)\n",
        "\n",
        "# Combine all features\n",
        "X = pd.concat([X_table, X_pl2324, X_encoded_teams], axis=1)\n",
        "\n",
        "# Convert all column names to strings\n",
        "X.columns = X.columns.astype(str)\n",
        "\n",
        "# Splitting Data into Training and Testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
        "\n",
        "# Impute missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit and transform on training data\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data using the same imputer\n",
        "X_test_imputed = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "9nrR_nFbmPSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building, Training and Prediction using Logistic Regression**"
      ],
      "metadata": {
        "id": "VD4ZG_bsd7mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the imputed test set\n",
        "y_pred = model.predict(X_test_imputed)"
      ],
      "metadata": {
        "id": "L0SRyTT5d-RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Performance of the Metrics"
      ],
      "metadata": {
        "id": "Z8FTgHzpeIO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "XgJc4JymeHrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the Output:**\n",
        "\n",
        "Accuracy: 0.9737\n",
        "\n",
        "Meaning: 97.37% of the predictions made by the model are correct. Accuracy is a good overall indicator of how well the model performs, especially when the classes are balanced.\n",
        "\n",
        "Interpretation: A high accuracy (close to 1) indicates that the model is performing well.\n",
        "\n",
        "Precision: 0.9743\n",
        "\n",
        "Meaning: Of the instances predicted as positive (correctly classified), 97.43% are truly positive. Precision is particularly important when the cost of false positives is high.\n",
        "\n",
        "Interpretation: High precision means the model is not often wrong when it predicts a particular class.\n",
        "\n",
        "Recall: 0.9737\n",
        "\n",
        "Meaning: Of all the actual positive instances, the model correctly identified 97.37%. Recall is crucial when the cost of false negatives is high.\n",
        "\n",
        "Interpretation: High recall indicates that the model is capturing most of the true positive cases.\n",
        "\n",
        "F1-Score: 0.9737\n",
        "\n",
        "Meaning: The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially useful when the classes are imbalanced.\n",
        "\n",
        "Interpretation: A high F1-score suggests the model has a good balance between precision and recall.\n",
        "\n",
        "**Interpretation for Confusion Matrix:**\n",
        "\n",
        "Class A (26 instances total): 25 were correctly classified as A. 1 was misclassified as H.\n",
        "\n",
        "Class D (19 instances total): All 19 were correctly classified as D.\n",
        "\n",
        "Class H (31 instances total): 30 were correctly classified as H. 1 was misclassified as D.\n",
        "\n",
        "Overall: The model performs well across all classes, with very few misclassifications.\n",
        "\n",
        "Classification Report:\n",
        "\n",
        "Class A: Precision: 1.00: Perfect precision, no false positives. Recall: 0.96: Missed 1 true positive. F1-Score: 0.98: High overall performance for Class A.\n",
        "\n",
        "Class D: Precision: 0.95: Few false positives. Recall: 1.00: Captured all true positives. F1-Score: 0.97: High overall performance for Class D.\n",
        "\n",
        "Class H: Precision: 0.97: Few false positives. Recall: 0.97: Missed 1 true positive. F1-Score: 0.97: High overall performance for Class H."
      ],
      "metadata": {
        "id": "c9LkcCdEec1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model using Random Forest"
      ],
      "metadata": {
        "id": "-sI1kSWpLq3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERFORMING LOGISTIC REGRESSION HYPERPARAMETERS**"
      ],
      "metadata": {
        "id": "2ZXi8bzPjlQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=32)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "# Fit the model to find the best hyperparameters\n",
        "grid_search.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_log_reg = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred_best = best_log_reg.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the best model\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(\"Accuracy of Best Model:\", accuracy_best)"
      ],
      "metadata": {
        "id": "1VoJrvmDjnMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building, Training, Testing and Prediction using Random Forest**"
      ],
      "metadata": {
        "id": "rAV8EKdrkSem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Random Forest Classifier\n",
        "# rf_classifier = RandomForestClassifier(random_state=42)\n",
        "# rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# # Predictions\n",
        "# y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# # Evaluation\n",
        "# print(\"Random Forest Classifier Performance:\")\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "# print(\"Precision:\", precision_score(y_test, y_pred_rf, average='macro'))\n",
        "# print(\"Recall:\", recall_score(y_test, y_pred_rf, average='macro'))\n",
        "# print(\"F1 Score:\", f1_score(y_test, y_pred_rf, average='macro'))\n",
        "# print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "o0oPSVLrJesI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Train the model for random Forest\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test_imputed)\n",
        "\n",
        "# Calculate the Metrics Accuracy, Precision, Recall and F1 score\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "class_report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"Precision:\", precision_rf)\n",
        "print(\"Recall:\", recall_rf)\n",
        "print(\"F1-score:\", f1_rf)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_rf)\n",
        "print(\"\\nClassification Report:\\n\", class_report_rf)"
      ],
      "metadata": {
        "id": "VdekM9Gik72v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the Output:**\n",
        "\n",
        "**Accuracy: 0.8421**\n",
        "\n",
        "Meaning: 84.21% of the predictions made by the model are correct. Accuracy is an overall measure of how well the model performs.\n",
        "\n",
        "Interpretation: An accuracy of 84.21% indicates that the model is performing well but not perfectly. It correctly classifies most instances but makes some mistakes.\n",
        "\n",
        "**Precision: 0.8406**\n",
        "\n",
        "Meaning: On average, 84.06% of the instances predicted as a certain class are truly that class. Precision measures the accuracy of the positive predictions.\n",
        "\n",
        "Interpretation: This suggests that when the model predicts a certain class, it's correct most of the time.\n",
        "\n",
        "**Recall: 0.8421**\n",
        "\n",
        "Meaning: On average, 84.21% of the actual positive instances are correctly identified by the model. Recall measures how well the model captures the true positives.\n",
        "\n",
        "Interpretation: The model is fairly good at identifying the true instances of each class.\n",
        "\n",
        "**F1-Score: 0.8302**\n",
        "\n",
        "Meaning: The F1-score is the harmonic mean of precision and recall. It balances the two metrics and is especially useful when there is an uneven class distribution.\n",
        "\n",
        "Interpretation: An F1-score of 0.83 indicates that the model has a good balance between precision and recall but leaves room for improvement.\n",
        "\n",
        "**Interpretation for Confusion Matrix:**\n",
        "\n",
        "Class A (26 instances total): 23 were correctly classified as A. 2 were misclassified as D. 1 was misclassified as H.\n",
        "\n",
        "Class D (19 instances total): 10 were correctly classified as D. 5 were misclassified as A. 4 were misclassified as H.\n",
        "\n",
        "Class H (31 instances total): All 31 were correctly classified as H. Overall: The model performs well for Class H but struggles with Class D, with some misclassifications.\n",
        "\n",
        "**Classification Report:**\n",
        "\n",
        "**Class A:** Precision: 0.82: There are some false positives, but the model generally predicts Class A correctly. Recall: 0.88: Most actual instances of Class A are correctly identified. F1-Score: 0.85: Indicates a good balance between precision and recall for Class A.\n",
        "\n",
        "**Class D:** Precision: 0.83: There are false positives when predicting Class D. Recall: 0.53: The model misses quite a few actual instances of Class D. F1-Score: 0.65: Lower performance for Class D compared to A and H, indicating that the model struggles with this class.\n",
        "\n",
        "**Class H:** Precision: 0.86: Very few false positives for Class H. Recall: 1.00: All actual instances of Class H are correctly identified. F1-Score: 0.93: Strong performance for Class H, indicating the model handles this class well."
      ],
      "metadata": {
        "id": "Jbe-kiCYlXqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**PERFORMING RANDOM FOREST HYPERPARAMETERS**"
      ],
      "metadata": {
        "id": "O5zrtKIzmItR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV - you can use RandomizedSearchCV for a faster search\n",
        "# grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "grid_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=5, n_jobs=-1, verbose=2, scoring='accuracy', random_state=42)\n",
        "\n",
        "# Fit the model to find the best hyperparameters\n",
        "grid_search.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred_best_rf = best_rf_classifier.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the best model\n",
        "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
        "precision_best_rf = precision_score(y_test, y_pred_best_rf, average='weighted')\n",
        "recall_best_rf = recall_score(y_test, y_pred_best_rf, average='weighted')\n",
        "f1_best_rf = f1_score(y_test, y_pred_best_rf, average='weighted')\n",
        "conf_matrix_best_rf = confusion_matrix(y_test, y_pred_best_rf)\n",
        "class_report_best_rf = classification_report(y_test, y_pred_best_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy_best_rf)\n",
        "print(\"Precision:\", precision_best_rf)\n",
        "print(\"Recall:\", recall_best_rf)\n",
        "print(\"F1-score:\", f1_best_rf)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_best_rf)\n",
        "print(\"\\nClassification Report:\\n\", class_report_best_rf)"
      ],
      "metadata": {
        "id": "st4sT6g4mYsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model using Support Vector Machine"
      ],
      "metadata": {
        "id": "wqWicjfqOZFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Support Vector Machine classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_classifier.predict(X_test_imputed)\n",
        "\n",
        "# Calculate the metrics\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "precision_svm = precision_score(y_test, y_pred_svm, average='weighted')\n",
        "recall_svm = recall_score(y_test, y_pred_svm, average='weighted')\n",
        "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
        "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
        "class_report_svm = classification_report(y_test, y_pred_svm)\n",
        "\n",
        "# Print the results\n",
        "print(\"SVM Model Performance:\")\n",
        "print(\"Accuracy:\", accuracy_svm)\n",
        "print(\"Precision:\", precision_svm)\n",
        "print(\"Recall:\", recall_svm)\n",
        "print(\"F1-score:\", f1_svm)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_svm)\n",
        "print(\"\\nClassification Report:\\n\", class_report_svm)\n"
      ],
      "metadata": {
        "id": "uAJcvPbJO0lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Model Performance Output Interpretation\n",
        "1. Overall Model Performance\n",
        "\tAccuracy: 0.974 (97.37%)\n",
        "o\tInterpretation: The model correctly predicted 97.37% of the instances in the test set. This high accuracy indicates that the SVM model is performing well in classifying the data into their respective categories.\n",
        "\tPrecision: 0.974 (97.43%)\n",
        "o\tInterpretation: Precision is the ratio of correctly predicted positive observations to the total predicted positives. A precision of 97.43% means that when the model predicts a certain class, it is correct 97.43% of the time on average across all classes. This suggests that the model has a low rate of false positives.\n",
        "\tRecall: 0.974 (97.37%)\n",
        "o\tInterpretation: Recall is the ratio of correctly predicted positive observations to all observations in the actual class. A recall of 97.37% indicates that the model is able to correctly identify 97.37% of the actual positive cases across all classes, implying a low rate of false negatives.\n",
        "\tF1-Score: 0.974 (97.37%)\n",
        "o\tInterpretation: The F1-score is the harmonic mean of precision and recall, providing a balanced measure of the models performance. With an F1-score of 97.37%, the model shows strong performance in terms of both precision and recall.\n",
        "2. Confusion Matrix Analysis\n",
        "\tClass A:\n",
        "\tTrue Positives (TP): 25\n",
        "\tFalse Negatives (FN): 1\n",
        "\tFalse Positives (FP): 0\n",
        "\tInterpretation: Out of 26 instances of Class A, the model correctly identified 25. There was 1 instance where Class A was misclassified as Class H. The model never falsely predicted Class A when it was not actually A.\n",
        "\tClass D:\n",
        "\tTrue Positives (TP): 19\n",
        "\tFalse Negatives (FN): 0\n",
        "\tFalse Positives (FP): 0\n",
        "\tInterpretation: The model perfectly identified all instances of Class D with no false negatives or false positives, indicating that the model is highly effective at recognizing this class.\n",
        "\tClass H:\n",
        "\tTrue Positives (TP): 30\n",
        "\tFalse Negatives (FN): 1\n",
        "\tFalse Positives (FP): 1\n",
        "\tInterpretation: Out of 31 instances of Class H, the model correctly identified 30. There was 1 instance where Class H was misclassified as Class A, and 1 instance of Class A was incorrectly classified as Class H. This suggests a very slight confusion between Class A and Class H.\n",
        "3. Detailed Classification Report\n",
        "The classification report provides precision, recall, and F1-score for each individual class:\n",
        "\tClass A:\n",
        "o\tPrecision: 1.00 (100%)\n",
        "\tThe model's predictions for Class A are completely accurate when it predicts Class A.\n",
        "o\tRecall: 0.96 (96%)\n",
        "\tThe model correctly identified 96% of all actual Class A instances.\n",
        "o\tF1-score: 0.98 (98%)\n",
        "\tThe overall performance for Class A is very high, with an excellent balance between precision and recall.\n",
        "\tClass D:\n",
        "o\tPrecision: 0.95 (95%)\n",
        "\tWhen the model predicts Class D, it is correct 95% of the time.\n",
        "o\tRecall: 1.00 (100%)\n",
        "\tThe model identified all Class D instances correctly.\n",
        "o\tF1-score: 0.97 (97%)\n",
        "\tThis score shows the model has strong performance with very high precision and perfect recall.\n",
        "\tClass H:\n",
        "o\tPrecision: 0.97 (97%)\n",
        "\tThe models predictions for Class H are correct 97% of the time.\n",
        "o\tRecall: 0.97 (97%)\n",
        "\tThe model correctly identified 97% of all actual Class H instances.\n",
        "o\tF1-score: 0.97 (97%)\n",
        "\tThis indicates consistently high performance in predicting Class H.\n",
        "4. Macro and Weighted Averages\n",
        "\tMacro Average:\n",
        "o\tPrecision: 0.97\n",
        "o\tRecall: 0.98\n",
        "o\tF1-score: 0.97\n",
        "o\tInterpretation: The macro average is the unweighted mean of precision, recall, and F1-score across all classes. These values indicate the model performs uniformly well across all classes without giving preference to any particular class.\n",
        "\tWeighted Average:\n",
        "o\tPrecision: 0.97\n",
        "o\tRecall: 0.97\n",
        "o\tF1-score: 0.97\n",
        "o\tInterpretation: The weighted average accounts for the support (the number of true instances for each class) when calculating the mean precision, recall, and F1-score. The high values here indicate that the model's overall performance is consistently strong, even when considering the class distribution.\n",
        "Conclusion\n",
        "The SVM model demonstrates excellent performance across all metrics. It achieves high accuracy, precision, recall, and F1-scores, indicating it is well-suited for the classification task at hand. The confusion matrix shows minimal misclassification, and the detailed classification report confirms that the model handles all classes effectively. The model's performance suggests it is a reliable tool for predicting the classes in the given dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "FKt5DkdZxjp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],            # Regularization parameter\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],    # Kernel coefficient\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid,\n",
        "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV to the data\n",
        "grid_search.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Retrieve the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_svm_best = best_svm_model.predict(X_test_imputed)\n",
        "\n",
        "# Calculate the metrics for the best model\n",
        "accuracy_svm_best = accuracy_score(y_test, y_pred_svm_best)\n",
        "precision_svm_best = precision_score(y_test, y_pred_svm_best, average='weighted')\n",
        "recall_svm_best = recall_score(y_test, y_pred_svm_best, average='weighted')\n",
        "f1_svm_best = f1_score(y_test, y_pred_svm_best, average='weighted')\n",
        "conf_matrix_svm_best = confusion_matrix(y_test, y_pred_svm_best)\n",
        "class_report_svm_best = classification_report(y_test, y_pred_svm_best)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Print the results of the best model\n",
        "print(\"Best SVM Model Performance:\")\n",
        "print(\"Accuracy:\", accuracy_svm_best)\n",
        "print(\"Precision:\", precision_svm_best)\n",
        "print(\"Recall:\", recall_svm_best)\n",
        "print(\"F1-score:\", f1_svm_best)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_svm_best)\n",
        "print(\"\\nClassification Report:\\n\", class_report_svm_best)\n"
      ],
      "metadata": {
        "id": "TkGF2ojny9pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of SVM Model Performance\n",
        "After hyperparameter tuning, the SVM model achieved outstanding performance:\n",
        "\n",
        "Best Hyperparameters:\n",
        "\n",
        "C: 100, gamma: 0.001, kernel: 'rbf'\n",
        "These parameters were selected to maximize the model's accuracy and ability to generalize.\n",
        "Overall Metrics:\n",
        "\n",
        "Accuracy: 98.68%\n",
        "Precision: 98.73%\n",
        "Recall: 98.68%\n",
        "F1-Score: 98.68%\n",
        "These high metrics indicate the model performs exceptionally well, with very few misclassifications.\n",
        "Confusion Matrix:\n",
        "\n",
        "The model made only two minor errors, misclassifying one instance each between classes A and H, but perfectly identified all instances of Class D.\n",
        "Classification Report:\n",
        "\n",
        "Class A: Precision and recall are nearly perfect.\n",
        "Class D: Flawless performance with 100% precision, recall, and F1-score.\n",
        "Class H: Slightly lower precision but perfect recall, indicating strong overall performance.\n",
        "Macro and Weighted Averages:\n",
        "\n",
        "Both macro and weighted averages for precision, recall, and F1-score are extremely high at 99%, demonstrating consistent performance across all classes."
      ],
      "metadata": {
        "id": "7FuTYvpZ1Atq"
      }
    }
  ]
}