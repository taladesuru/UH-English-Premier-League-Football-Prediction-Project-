{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taladesuru/UH-English-Premier-League-Football-Prediction-Project-/blob/main/English%20premier%20League%20prediction%3A%20The%20Analysis%20of%20The%20Relevance%20of%20Football%20Teams%E2%80%99%20Home%20Advantage%20Point%20Accumulation__V5_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive To Access Dataset"
      ],
      "metadata": {
        "id": "s7DtuZ_UWZGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D-x--AG0bga5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Library Importation"
      ],
      "metadata": {
        "id": "jCVO8KnjWpN3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBDeb6NnbUVS"
      },
      "outputs": [],
      "source": [
        "#Import the Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read The 2023-2024 premier league Dataset csv file into the pl2324 DataFrame\n",
        "pl2324 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/2023-2024.csv')"
      ],
      "metadata": {
        "id": "knpyO7a4cDCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display The top  5 and bottom 5 rows of the pl2324 dataframe\n",
        "pl2324"
      ],
      "metadata": {
        "id": "4GiD7m0rk5_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display The Summary of Information of the Dataset File\n",
        "pl2324.info()"
      ],
      "metadata": {
        "id": "-Adhslssk8J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA PREPROCESSING**"
      ],
      "metadata": {
        "id": "_HDB8a8Fl_KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA CLEANING"
      ],
      "metadata": {
        "id": "a_UxK9YdlZ7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and count for missing values\n",
        "pl2324.isnull().sum()"
      ],
      "metadata": {
        "id": "bRALq5Sjl18_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Date column to datetime format with the specified date format\n",
        "pl2324['Date'] = pd.to_datetime(pl2324['Date'], format='%d/%m/%Y')"
      ],
      "metadata": {
        "id": "TQJNRo2Ql4ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the datatype of the columns of the pl2324 DataFrame\n",
        "pl2324.dtypes"
      ],
      "metadata": {
        "id": "kvoe3F--hF-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with unique team names\n",
        "teams = pl2324['HomeTeam'].unique()\n",
        "pl2324_table = pd.DataFrame({'Team': teams})\n",
        "\n",
        "# Display the DataFrame\n",
        "pl2324_table"
      ],
      "metadata": {
        "id": "4Zz63he-Ivml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Dataframe"
      ],
      "metadata": {
        "id": "qaG_jDX1jixn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize relevant columns in the dataframe and the default value created is set to zero\n",
        "pl2324_table[['Played','Win','Draw','Loss','GF','GA','GD','Points']] = 0\n",
        "\n",
        "# Check Table\n",
        "pl2324_table"
      ],
      "metadata": {
        "id": "DHnAgRqAJ_bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index of pl2324_table to make 'Team' a regular column\n",
        "pl2324_table = pl2324_table.reset_index()\n",
        "\n",
        "# Set 'Team' as the new index\n",
        "pl2324_table = pl2324_table.set_index('Team')\n",
        "\n",
        "# Iterate over each row in the DataFrame pl2324\n",
        "for i, row in pl2324.iterrows():\n",
        "    home = row['HomeTeam']\n",
        "    away = row['AwayTeam']\n",
        "\n",
        "    # Update 'Played' count for home and away teams\n",
        "    pl2324_table.loc[home, 'Played'] += 1\n",
        "    pl2324_table.loc[away, 'Played'] += 1\n",
        "\n",
        "    # Update 'GF' (Goals For) and 'GA' (Goals Against) for home and away teams\n",
        "    pl2324_table.loc[home, 'GF'] += row['FTHG']\n",
        "    pl2324_table.loc[away, 'GF'] += row['FTAG']\n",
        "    pl2324_table.loc[home, 'GA'] += row['FTAG']\n",
        "    pl2324_table.loc[away, 'GA'] += row['FTHG']\n",
        "\n",
        "    # Update 'Win', 'Loss', 'Draw' counts based on match result\n",
        "    if row['FTR'] == \"H\":\n",
        "        pl2324_table.loc[home, 'Win'] += 1\n",
        "        pl2324_table.loc[away, 'Loss'] += 1\n",
        "    elif row['FTR'] == \"A\":\n",
        "        pl2324_table.loc[away, 'Win'] += 1\n",
        "        pl2324_table.loc[home, 'Loss'] += 1\n",
        "    else:\n",
        "        pl2324_table.loc[home, 'Draw'] += 1\n",
        "        pl2324_table.loc[away, 'Draw'] += 1\n",
        "\n",
        "# Calculate 'Points' as 3 times 'Win' plus 'Draw'\n",
        "pl2324_table['Points'] = 3 * pl2324_table['Win'] + pl2324_table['Draw']\n",
        "\n",
        "# Calculate 'GD' (Goal Difference) as 'GF' minus 'GA'\n",
        "pl2324_table['GD'] = pl2324_table['GF'] - pl2324_table['GA']\n",
        "\n",
        "# Sort teams by 'Points' in descending order\n",
        "pl2324_table = pl2324_table.sort_values(by='Points', ascending=False)\n",
        "\n",
        "# Reset the index to have a sequential index\n",
        "pl2324_table = pl2324_table.reset_index()\n",
        "\n",
        "pl2324_table"
      ],
      "metadata": {
        "id": "4UOF0PIvKhis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above updates the standings table pl2324_table for the league based on match results from the DataFrame pl2324, which calculates the number of games played, goals for (GF), goals against (GA), wins, losses, draws, points, and goal difference (GD) for each team.\n",
        "\n",
        "**Furthermore**\n",
        "\n",
        "1. **Reset and Set Index:** The pl2324_table index is reset to make Team a regular column, then the Team is set as the index.\n",
        "\n",
        "2. **Iterate Over Matches:** For each match in pl2324, the home and away teams are identified. The number of games played (Played), goals for (GF), goals against (GA), wins (Win), losses (Loss), and draws (Draw) are updated based on the match full time result (FTR).\n",
        "\n",
        "3. **Calculate Points and Goal Difference:** The Point is calculated as 3 points for each win plus 1 point for each draw, while the goal difference (GD) is calculated as goals for (GF) minus goals against (GA).\n",
        "\n",
        "4. **Sort and Reset Index:** Teams are sorted by Points in descending order, while the index is reset to ensure a sequential index."
      ],
      "metadata": {
        "id": "wldrMkUyjl0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(pl2324_table.Team)))\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Plot bars with unique colors\n",
        "for i, league_team in enumerate(pl2324_table.Team):\n",
        "    plt.bar(league_team, pl2324_table.Win[i], color=colors[i], label=league_team)\n",
        "\n",
        "plt.xlabel('League_teams')\n",
        "plt.ylabel('Wins')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Total Premier League Wins 2023-2024 Season')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fJ9XqR53R8oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(pl2324_table.Team)))\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Plot bars with unique colors\n",
        "for i, league_team in enumerate(pl2324_table.Team):\n",
        "    plt.bar(league_team, pl2324_table.Loss[i], color=colors[i], label=league_team)\n",
        "\n",
        "plt.xlabel('League_teams')\n",
        "plt.ylabel('Loss')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Total Premier League Losses 2023-2024')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3PBzl4rMWqxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(pl2324_table.Team)))\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Plot bars with unique colors\n",
        "for i, league_team in enumerate(pl2324_table.Team):\n",
        "    plt.bar(league_team, pl2324_table.Draw[i], color=colors[i], label=league_team)\n",
        "\n",
        "plt.xlabel('League_teams')\n",
        "plt.ylabel('Draw')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Total Premier League Draws 2023-2024')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), ncol=1, fontsize='small')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Gpup4b5XLe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324.head()"
      ],
      "metadata": {
        "id": "y3acklTPr1-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Home Game Total Counts\n",
        "pl2324.HomeTeam.value_counts()"
      ],
      "metadata": {
        "id": "B0YEtxKTr3Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Away Game Total Count\n",
        "pl2324.AwayTeam.value_counts()"
      ],
      "metadata": {
        "id": "mZU6dT3r1Edg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Missing Values\n",
        "missing_val = pl2324.isnull().sum()\n",
        "\n",
        "# Display Columns with Missing Values\n",
        "missing_columns = missing_val[missing_val > 0]\n",
        "missing_columns"
      ],
      "metadata": {
        "id": "Lun_gQUX1MVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following columns inside the dataset had missing values:\n",
        "\n",
        "\n",
        "Bet & Win home win odds (BWH), Bet & Win draw odds (BWD) and Bet & Win away win odds:\t2 missing values each\n",
        "\n",
        "Interwetten home win odds (IWH), Interwetten draw odds (IWD) and Interwetten away win odds (IWA):\t182 missing values each\n",
        "\n",
        "Pinnacle over 2.5 goals (P>2.5) and Pinnacle under 2.5 goals (P<2.5):\t8 missing values each\n",
        "\n",
        "Betting Win Chance Home (BWCH), Betting Win Chance Draw (BWCD) and Betting Win Chance Away (BWCA):\t12 missing values each\n",
        "\n",
        "Implied Win Chance Home (IWCH), Implied Win Chance Draw (IWCD) and Implied Win Chance Away (IWCA):\t182 missing values each\n",
        "\n",
        "Percentage Chance of Over 2.5 Goals (PC>2.5), Percentage Chance of under 2.5 Goals (PC<2.5):\t7 missing values each\n",
        "\n",
        "Maximum Closing Asian Handicap Home (MaxCAHH), Maximum Closing Asian Handicap Away (MaxCAHA)\t1 missing values each\n"
      ],
      "metadata": {
        "id": "n05njb1fa0jL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values for columns with a small number of missing values using the median statistics\n",
        "for col in ['BWH', 'BWD', 'BWA', 'P>2.5', 'P<2.5', 'BWCH', 'BWCD', 'BWCA', 'PC>2.5', 'PC<2.5', 'MaxCAHH', 'MaxCAHA']:\n",
        "    pl2324[col].fillna(pl2324[col].median(), inplace=True)\n",
        "\n",
        "# Drop columns with a large number of missing values\n",
        "pl2324.drop(columns=['IWH', 'IWD', 'IWA', 'IWCH', 'IWCD', 'IWCA'], inplace=True)\n",
        "\n",
        "# Select relevant features for analysis\n",
        "select_columns = ['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG'] + \\\n",
        "                   [col for col in pl2324.columns if 'B365' in col or 'Max' in col or 'Avg' in col]\n",
        "\n",
        "select_data = pl2324[select_columns]\n",
        "\n",
        "# Display the first few rows of the preprocessed dataset\n",
        "select_data\n"
      ],
      "metadata": {
        "id": "MD1qBWvyaL5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLORATORY DATA ANALYSIS"
      ],
      "metadata": {
        "id": "ygxLxZOEi83j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the Data Statistics for pl2324_table\n",
        "data_stats = pl2324_table.describe()\n",
        "\n",
        "# Calculate mode\n",
        "mode_values = pl2324_table.mode().iloc[0]\n",
        "\n",
        "# Display results\n",
        "print(\"Data Statistics:\\n\", data_stats)\n",
        "print(\"\\nMode Values:\\n\", mode_values)"
      ],
      "metadata": {
        "id": "Cxszz88qE-Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate goals scored at Home by each Team\n",
        "home_goals_by_team = pl2324.groupby('HomeTeam')['FTHG'].sum().reset_index()\n",
        "\n",
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(home_goals_by_team)))\n",
        "\n",
        "# Create a bar chart with unique colors\n",
        "plt.figure(figsize=(15, 7))\n",
        "for i, team in enumerate(home_goals_by_team['HomeTeam']):\n",
        "    plt.bar(team, home_goals_by_team['FTHG'][i], color=colors[i], label=team)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Total Home Goals Scored by Each Team')\n",
        "plt.xlabel('Home Team')\n",
        "plt.ylabel('Total Home Goals')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), ncol=1, fontsize='small')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XREGZjAGcSMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate goals conceded at Home by each Team\n",
        "home_goals_conceded_by_team = pl2324.groupby('HomeTeam')['FTAG'].sum().reset_index()\n",
        "\n",
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(home_goals_conceded_by_team)))\n",
        "\n",
        "# Create a bar chart with unique colors\n",
        "plt.figure(figsize=(15, 7))\n",
        "for i, team in enumerate(home_goals_conceded_by_team['HomeTeam']):\n",
        "    plt.bar(team, home_goals_conceded_by_team['FTAG'][i], color=colors[i], label=team)\n",
        "\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Total Home Goals Conceded by Each Team')\n",
        "plt.xlabel('Home Team')\n",
        "plt.ylabel('Total Home Goals Conceded')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), ncol=1, fontsize='small')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BtySTnE_FMOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate goals scored at home and goals conceded by each team\n",
        "home_goals_by_team = pl2324_table.groupby('Team')[['GF', 'GA']].sum().reset_index()\n",
        "\n",
        "# Generate unique colors for each team\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(home_goals_by_team)))\n",
        "\n",
        "# Create a bar chart with unique colors\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Plotting two bars for each team: one for goals scored (GF) and one for goals conceded (GA)\n",
        "bar_width = 0.35\n",
        "x = np.arange(len(home_goals_by_team['Team']))\n",
        "\n",
        "# Plot Goals For\n",
        "plt.bar(x - bar_width/2, home_goals_by_team['GF'], width=bar_width, color='blue', label='Goals For')\n",
        "\n",
        "# Plot Goals Against\n",
        "plt.bar(x + bar_width/2, home_goals_by_team['GA'], width=bar_width, color='red', label='Goals Against')\n",
        "\n",
        "# Setting the x-ticks to team names\n",
        "plt.xticks(x, home_goals_by_team['Team'], rotation=90)\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Total Home Goals Scored and Conceded by Each Team')\n",
        "plt.xlabel('Home Team')\n",
        "plt.ylabel('Total Goals')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f7d6BFwz5tLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=pl2324, x='FTR', palette='Set2', order=['H', 'A', 'D'])\n",
        "plt.xticks(ticks=[0, 1, 2], labels=['Home Win', 'Away Win', 'Draw'])\n",
        "plt.title('Plot of Full Time Result Counts for 2023 - 2024 Season')\n",
        "plt.xlabel('Full Time Result')\n",
        "plt.ylabel('Count of Matches')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f0PyaHGJS0rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table.head()"
      ],
      "metadata": {
        "id": "oegac2I12TAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table.describe()"
      ],
      "metadata": {
        "id": "MOaI7bQnq6XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = pl2324_table[['Played', 'Win', 'Draw', 'Loss', 'GF', 'GA', 'GD', 'Points']].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "correlation_matrix"
      ],
      "metadata": {
        "id": "iDUQI0pqoAEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation Matrix Analysis**\n",
        "\n",
        "**Positive Correlations**\n",
        "\n",
        "•\tGoals For (GF) and Wins: A high correlation (0.91) signified that the teams that scored more goals were likely to win more matches.\n",
        "\n",
        "•\tGoals For (GF) and Points: A strong positive correlation (0.91) meant that teams that scored more goal were likely to have more wins and consequently earned more points. This was because the more goals scored, the higher it is crucial for winning and earning more points.\n",
        "\n",
        "•\tPoints and Wins: A very strong correlation (1) suggests that teams with more points also had more wins.\n",
        "\n",
        "•\tPoints and Goal Difference (GD): A strong correlation (0.97) indicated that teams with a higher goal difference earned more points.\n",
        "\n",
        "**Negative Correlations**\n",
        "\n",
        "•\tLoss and Points: A strong negative correlation (-0.99) showed that teams with more losses had fewer points. This was expected as losses meant fewer points were earned.\n",
        "\n",
        "•\tLoss and Goals For (GF): The negative correlation (-0.97) suggests that teams that lost more matches scored fewer goals.\n",
        "\n",
        "•\tGoal Difference (GD) and Losses: The negative correlation (-0.92) meant teams with larger goal difference were less likely to suffer losses.\n",
        "\n",
        "The most crucial revelation from the correlation matrix figure was the observation that goals are crucial. The strong correlations between goals scored (GF) and wins, points, and goal difference highlighted the significant role goals played in each team’s success. While also revealing that losses are detrimental to team’s survival in the league.\n",
        "The negative correlations between losses and points, goals scored, and goal difference underscored the importance of minimizing losses. Overall, the correlation matrix provided a clear picture of the strong relationships between the premier league team's goals, wins, losses, goal difference, and ultimately, their points earned. It gave emphasis to the importance of scoring goals and minimizing losses to achieve success and prevent relegation."
      ],
      "metadata": {
        "id": "xvWgYwnmjiO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "Cmcctc-j10ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze if teams with more Home game points do better in the overall league standing at the end of the season (38 games)\n"
      ],
      "metadata": {
        "id": "eWWkyUcXFpFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new features for Home Game Points\n",
        "pl2324['HomePoints'] = np.where(pl2324['FTR'] == 'H', 3, np.where(pl2324['FTR'] == 'D', 1, 0))\n",
        "\n",
        "# Aggregate Home Game Points for each team\n",
        "home_points = pl2324.groupby('HomeTeam').agg({'HomePoints': 'sum'}).reset_index()\n",
        "home_points.rename(columns={'HomeTeam': 'Team'}, inplace=True)\n",
        "\n",
        "# Put the HomePoint in an Descending Order\n",
        "home_points = home_points.sort_values(by='HomePoints', ascending=False)\n",
        "\n",
        "print(home_points)"
      ],
      "metadata": {
        "id": "Xhghc7uQESkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by HomePoints in descending order to determine FinalStanding\n",
        "home_points = home_points.sort_values(by='HomePoints', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Assign FinalStanding based on sorted order\n",
        "home_points['HomeGameFinalStanding'] = home_points.index + 1\n",
        "\n",
        "# Display the adjusted standings\n",
        "print(home_points)"
      ],
      "metadata": {
        "id": "dfWSdjrP7QYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot to visualize the relationship\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='HomePoints', y='HomeGameFinalStanding', data=home_points, hue='Team', palette='tab20', s=100)\n",
        "plt.xlabel('Home Game Points')\n",
        "plt.ylabel('Final Standing (Lower is Better)')\n",
        "plt.title('Home Game Points vs Home Game Final Standing')\n",
        "plt.gca().invert_yaxis() # Invert the y-axis to show lower standing as better\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "correlation = home_points['HomePoints'].corr(home_points['HomeGameFinalStanding'])\n",
        "print('\\n')\n",
        "print(f'Correlation between Home Game Points and Home Game Final Standing: {correlation}')\n"
      ],
      "metadata": {
        "id": "GJ5c5wYGAryK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding New Features to the pl2324_table Dataframe"
      ],
      "metadata": {
        "id": "2JvmUl7dRuDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding features to Build and Analyze our Model\n",
        "pl2324_table[['shots','shots_on_targets','shots_against','shots_on_targets_against']] = 0\n",
        "\n",
        "# Set the 'Team' column as the index BEFORE the loop\n",
        "pl2324_table = pl2324_table.set_index('Team')\n",
        "\n",
        "pl2324_table.head()\n",
        "\n",
        "for i in pl2324.index:\n",
        "    home = pl2324.HomeTeam.loc[i]\n",
        "    away = pl2324.AwayTeam.loc[i]\n",
        "    pl2324_table.loc[home,'shots'] += pl2324.HS.loc[i]\n",
        "    pl2324_table.loc[home,'shots_on_targets'] += pl2324.HST.loc[i]\n",
        "    pl2324_table.loc[away,'shots'] += pl2324.AS.loc[i]\n",
        "    pl2324_table.loc[away,'shots_on_targets'] += pl2324.AST.loc[i]\n",
        "    pl2324_table.loc[home,'shots_against'] += pl2324.AS.loc[i]\n",
        "    pl2324_table.loc[home,'shots_on_targets_against'] += pl2324.AST.loc[i]\n",
        "    pl2324_table.loc[away,'shots_against'] += pl2324.HS.loc[i]\n",
        "    pl2324_table.loc[away,'shots_on_targets_against'] += pl2324.HST.loc[i]\n",
        "pl2324_table = pl2324_table.reset_index()\n",
        "pl2324_table.head()"
      ],
      "metadata": {
        "id": "ABM96PqdRr_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Stages to get the Performance Rate for each Team**\n",
        "\n",
        "(A) Goal Difference (GD)"
      ],
      "metadata": {
        "id": "-8mDb5FySTqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Sort by Goal Difference\n",
        "pl2324_table = pl2324_table.sort_values(by='GD', ascending=False)\n",
        "\n",
        "print(pl2324_table[['Team', 'GD']])\n",
        "print('\\n')\n",
        "\n",
        "# Getting the unique color for each Team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Plot a Barchart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Team', y='GD', data=pl2324_table, palette=palette)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Team')\n",
        "plt.ylabel('Goal Difference')\n",
        "plt.title('Goal Difference by Team')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwwxRB74giOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(B) **Points Per Game (PPG)**: This normalizes performance across the numerous numbers of matches played."
      ],
      "metadata": {
        "id": "85shzr7vTIMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Points Per Game\n",
        "pl2324_table['PPG'] = pl2324_table['Points'] / pl2324_table['Played']\n",
        "\n",
        "# Sort by Points Per Game\n",
        "pl2324_table = pl2324_table.sort_values(by='PPG', ascending=False)\n",
        "\n",
        "print(pl2324_table[['Team', 'PPG']])\n",
        "print('\\n')\n",
        "\n",
        "# Getting the unique color for each Team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Plot a Barchart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Team', y='PPG', data=pl2324_table, palette=palette)  # Apply the custom palette here\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Team')\n",
        "plt.ylabel('Points Per Game')\n",
        "plt.title('Points Per Game by Team')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZPy_NiSATguK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C) Shot Conversion Rate (SCR): This measures how efficiently a team turns shots into goals"
      ],
      "metadata": {
        "id": "CpkDmvd4U6BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Shot Conversion Rate (Goals Scored / Total Shots)\n",
        "pl2324_table['SCR'] = pl2324_table['GF'] / pl2324_table['shots']\n",
        "\n",
        "# Handle potential division by zero (teams with no shots)\n",
        "pl2324_table['SCR'] = pl2324_table['SCR'].fillna(0)\n",
        "\n",
        "# Sort by Shot Conversion Rate\n",
        "pl2324_table = pl2324_table.sort_values(by='SCR', ascending=False)\n",
        "\n",
        "print(pl2324_table[['Team', 'SCR']])\n",
        "print('\\n')\n",
        "\n",
        "# Getting the unique color for each Team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Plot a Barchart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Team', y='SCR', data=pl2324_table, palette=palette)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Team')\n",
        "plt.ylabel('Shot Conversion Rate')\n",
        "plt.title('Shot Conversion Rate by Team')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6MJXXaOMU5HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(D) Weighing the Performance Index of each Team"
      ],
      "metadata": {
        "id": "6rbF37LKViur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weights\n",
        "weight_GD = 0.5\n",
        "weight_PPG = 0.3\n",
        "weight_SCR = 0.2\n",
        "\n",
        "# Calculate a weighted performance index\n",
        "pl2324_table['PerformanceIndex'] = (weight_GD * pl2324_table['GD']) + \\\n",
        "                               (weight_PPG * pl2324_table['PPG']) + \\\n",
        "                               (weight_SCR * pl2324_table['SCR'])\n",
        "\n",
        "# Sort by Performance Index\n",
        "pl2324_table = pl2324_table.sort_values(by='PerformanceIndex', ascending=False)\n",
        "\n",
        "print(pl2324_table[['Team', 'PerformanceIndex']])\n",
        "print('\\n')\n",
        "\n",
        "# Getting the unique color for each Team\n",
        "unique_teams = pl2324_table['Team'].unique()\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(unique_teams)))\n",
        "palette = dict(zip(unique_teams, colors))\n",
        "\n",
        "# Plot a Barchart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Team', y='PerformanceIndex', data=pl2324_table, palette=palette)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Team')\n",
        "plt.ylabel('Performance Index')\n",
        "plt.title('Performance Index by Team')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ue-l9xw9Vjzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table\n"
      ],
      "metadata": {
        "id": "r4Apuce4WpOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILDING MODEL and ANALYSIS"
      ],
      "metadata": {
        "id": "mmNyuhYm96JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "_BpBWTruspnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting Relevant Features and Target to use"
      ],
      "metadata": {
        "id": "o0POq2TYJ4pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pl2324_table.columns.unique()"
      ],
      "metadata": {
        "id": "xARMoBspmPsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the unique columns of the dataset\n",
        "pl2324.columns.unique()"
      ],
      "metadata": {
        "id": "pHXDQ9wOKJUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features and target\n",
        "features_table = ['Played', 'Win', 'Draw', 'Loss', 'GF', 'GA', 'GD', 'Points',\n",
        "                 'shots', 'shots_on_targets', 'shots_against', 'shots_on_targets_against',\n",
        "                 'PPG', 'SCR', 'PerformanceIndex']\n",
        "\n",
        "features_pl2324 = ['HTHG', 'HTAG', 'FTHG', 'FTAG', 'HS', 'AS', 'HST', 'AST']\n",
        "target = 'FTR'\n",
        "\n",
        "# Extract features and target variable\n",
        "X_table = pl2324_table[features_table]\n",
        "X_pl2324 = pl2324[features_pl2324]\n",
        "y = pl2324['FTR']\n",
        "\n",
        "# One-Hot Encoding for 'HomeTeam' and 'AwayTeam'\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "encoded_teams = encoder.fit_transform(pl2324[['HomeTeam', 'AwayTeam']]).toarray()\n",
        "\n",
        "# Convert the NumPy array to a DataFrame\n",
        "X_encoded_teams = pd.DataFrame(encoded_teams)\n",
        "\n",
        "# Combine all features\n",
        "X = pd.concat([X_table, X_pl2324, X_encoded_teams], axis=1)\n",
        "\n",
        "# Convert all column names to strings\n",
        "X.columns = X.columns.astype(str)\n",
        "\n",
        "# Splitting Data into Training and Testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
        "\n",
        "# Impute missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit and transform on training data\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data using the same imputer\n",
        "X_test_imputed = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "9nrR_nFbmPSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building, Training and Prediction using Logistic Regression**"
      ],
      "metadata": {
        "id": "VD4ZG_bsd7mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the imputed test set\n",
        "y_pred = model.predict(X_test_imputed)"
      ],
      "metadata": {
        "id": "L0SRyTT5d-RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Performance of the Metrics"
      ],
      "metadata": {
        "id": "Z8FTgHzpeIO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "XgJc4JymeHrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy: 0.9737**\n",
        "\n",
        "The 97.37% of the predictions made by the model are correct. Meaning that the model performed well in terms of accuracy because accuracy is a good overall indicator of model performance, especially when the classes are balanced. Furthermore, the high accuracy (close to 1) indicated that the model is performed well.\n",
        "\n",
        " **Precision: 0.9743**\n",
        "\n",
        "Of the instances predicted as positive (correctly classified), 97.43% are truly positive. Precision is particularly important when the cost of false positives is high. The high precision meant that the model is not often wrong when it predicts a particular class.\n",
        "\n",
        "**Recall: 0.9737**\n",
        "\n",
        "The recall is around 97.37%, which indicates that the model correctly identified 97.37% of the actual instances of each class. High recall suggests the model effectively identifies the true positive instances. Recall is crucial when the cost of false negatives is high.\n",
        "F1-Score: 0.9737\n",
        "The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially useful when the classes are imbalanced. and the high F1-score suggests the model has a good balance between precision and recall.\n",
        "\n",
        "**Confusion Matrix:**\n",
        "\n",
        "Class A (26 instances total): 25 were correctly classified as A. 1 was misclassified as H.\n",
        "\n",
        "Class D (19 instances total): All 19 were correctly classified as D.\n",
        "\n",
        "Class H (31 instances total): 30 were correctly classified as H. 1 was misclassified as D.\n",
        "\n",
        "Overall: The model performed well across all classes, with very few misclassifications.\n",
        "\n",
        "The matrix show that the model has a very high accuracy for predicting home wins (H), with only 1 false positive and it correctly predicts all draws (D) with no errors and there was 1 false negative for away wins (A), meaning one away win was predicted incorrectly.\n",
        "\n",
        "**Classification Report:**\n",
        "Class A: Precision: 1.00: Perfect precision, no false positives. Recall: 0.96: Missed 1 true positive. F1-Score: 0.98: High overall performance for Class A.\n",
        "\n",
        "Class D: Precision: 0.95: Few false positives. Recall: 1.00: Captured all true positives. F1-Score: 0.97: High overall performance for Class D.\n",
        "\n",
        "Class H: Precision: 0.97: Few false positives. Recall: 0.97: Missed 1 true positive. F1-Score: 0.97: High overall performance for Class H.\n",
        "\n",
        "This meant that the model performed exceptionally well across all classes, with near-perfect precision and recall. Even though the slight decrease in recall for class A (away wins) suggests a minor issue with missing some away wins, but overall, the performance is very strong. Furthermore, the high precision and recall scores, combined with the high F1-score, indicated a robust model that makes very few mistakes, whether predicting the presence of a particular result (precision) or ensuring that it captures all relevant instances (recall). Finally, the balanced metrics suggest that the model is not biased toward any specific class, handling each class's predictions effectively.\n"
      ],
      "metadata": {
        "id": "c9LkcCdEec1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model using Random Forest"
      ],
      "metadata": {
        "id": "-sI1kSWpLq3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERFORMING LOGISTIC REGRESSION HYPERPARAMETERS**"
      ],
      "metadata": {
        "id": "2ZXi8bzPjlQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=32)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "# Fit the model to find the best hyperparameters\n",
        "grid_search.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_log_reg = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred_best = best_log_reg.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the best model\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(\"Accuracy of Best Model:\", accuracy_best)"
      ],
      "metadata": {
        "id": "1VoJrvmDjnMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building, Training, Testing and Prediction using Random Forest**"
      ],
      "metadata": {
        "id": "rAV8EKdrkSem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Train the model for random Forest\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test_imputed)\n",
        "\n",
        "# Calculate the Metrics Accuracy, Precision, Recall and F1 score\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "class_report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"Precision:\", precision_rf)\n",
        "print(\"Recall:\", recall_rf)\n",
        "print(\"F1-score:\", f1_rf)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_rf)\n",
        "print(\"\\nClassification Report:\\n\", class_report_rf)"
      ],
      "metadata": {
        "id": "VdekM9Gik72v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the Output:**\n",
        "\n",
        "**Accuracy: 0.8421**\n",
        "\n",
        "Meaning: 84.21% of the predictions made by the model are correct. Accuracy is an overall measure of how well the model performs.\n",
        "\n",
        "Interpretation: An accuracy of 84.21% indicates that the model is performing well but not perfectly. It correctly classifies most instances but makes some mistakes.\n",
        "\n",
        "**Precision: 0.8406**\n",
        "\n",
        "Meaning: On average, 84.06% of the instances predicted as a certain class are truly that class. Precision measures the accuracy of the positive predictions.\n",
        "\n",
        "Interpretation: This suggests that when the model predicts a certain class, it's correct most of the time.\n",
        "\n",
        "**Recall: 0.8421**\n",
        "\n",
        "Meaning: On average, 84.21% of the actual positive instances are correctly identified by the model. Recall measures how well the model captures the true positives.\n",
        "\n",
        "Interpretation: The model is fairly good at identifying the true instances of each class.\n",
        "\n",
        "**F1-Score: 0.8302**\n",
        "\n",
        "Meaning: The F1-score is the harmonic mean of precision and recall. It balances the two metrics and is especially useful when there is an uneven class distribution.\n",
        "\n",
        "Interpretation: An F1-score of 0.83 indicates that the model has a good balance between precision and recall but leaves room for improvement.\n",
        "\n",
        "**Interpretation for Confusion Matrix:**\n",
        "\n",
        "Class A (26 instances total): 23 were correctly classified as A. 2 were misclassified as D. 1 was misclassified as H.\n",
        "\n",
        "Class D (19 instances total): 10 were correctly classified as D. 5 were misclassified as A. 4 were misclassified as H.\n",
        "\n",
        "Class H (31 instances total): All 31 were correctly classified as H. Overall: The model performs well for Class H but struggles with Class D, with some misclassifications.\n",
        "\n",
        "**Classification Report:**\n",
        "\n",
        "**Class A:** Precision: 0.82: There are some false positives, but the model generally predicts Class A correctly. Recall: 0.88: Most actual instances of Class A are correctly identified. F1-Score: 0.85: Indicates a good balance between precision and recall for Class A.\n",
        "\n",
        "**Class D:** Precision: 0.83: There are false positives when predicting Class D. Recall: 0.53: The model misses quite a few actual instances of Class D. F1-Score: 0.65: Lower performance for Class D compared to A and H, indicating that the model struggles with this class.\n",
        "\n",
        "**Class H:** Precision: 0.86: Very few false positives for Class H. Recall: 1.00: All actual instances of Class H are correctly identified. F1-Score: 0.93: Strong performance for Class H, indicating the model handles this class well."
      ],
      "metadata": {
        "id": "Jbe-kiCYlXqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**PERFORMING RANDOM FOREST HYPERPARAMETERS**"
      ],
      "metadata": {
        "id": "O5zrtKIzmItR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV - you can use RandomizedSearchCV for a faster search\n",
        "# grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "grid_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=5, n_jobs=-1, verbose=2, scoring='accuracy', random_state=42)\n",
        "\n",
        "# Fit the model to find the best hyperparameters\n",
        "grid_search.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred_best_rf = best_rf_classifier.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the best model\n",
        "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
        "precision_best_rf = precision_score(y_test, y_pred_best_rf, average='weighted')\n",
        "recall_best_rf = recall_score(y_test, y_pred_best_rf, average='weighted')\n",
        "f1_best_rf = f1_score(y_test, y_pred_best_rf, average='weighted')\n",
        "conf_matrix_best_rf = confusion_matrix(y_test, y_pred_best_rf)\n",
        "class_report_best_rf = classification_report(y_test, y_pred_best_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy_best_rf)\n",
        "print(\"Precision:\", precision_best_rf)\n",
        "print(\"Recall:\", recall_best_rf)\n",
        "print(\"F1-score:\", f1_best_rf)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_best_rf)\n",
        "print(\"\\nClassification Report:\\n\", class_report_best_rf)"
      ],
      "metadata": {
        "id": "st4sT6g4mYsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model using Support Vector Machine"
      ],
      "metadata": {
        "id": "wqWicjfqOZFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Support Vector Machine classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_classifier.predict(X_test_imputed)\n",
        "\n",
        "# Calculate the metrics\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "precision_svm = precision_score(y_test, y_pred_svm, average='weighted')\n",
        "recall_svm = recall_score(y_test, y_pred_svm, average='weighted')\n",
        "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
        "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
        "class_report_svm = classification_report(y_test, y_pred_svm)\n",
        "\n",
        "# Print the results\n",
        "print(\"SVM Model Performance:\")\n",
        "print(\"Accuracy:\", accuracy_svm)\n",
        "print(\"Precision:\", precision_svm)\n",
        "print(\"Recall:\", recall_svm)\n",
        "print(\"F1-score:\", f1_svm)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_svm)\n",
        "print(\"\\nClassification Report:\\n\", class_report_svm)\n"
      ],
      "metadata": {
        "id": "uAJcvPbJO0lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Model Performance Output Interpretation\n",
        "1. Overall Model Performance\n",
        "•\tAccuracy: 0.974 (97.37%)\n",
        "o\tInterpretation: The model correctly predicted 97.37% of the instances in the test set. This high accuracy indicates that the SVM model is performing well in classifying the data into their respective categories.\n",
        "•\tPrecision: 0.974 (97.43%)\n",
        "o\tInterpretation: Precision is the ratio of correctly predicted positive observations to the total predicted positives. A precision of 97.43% means that when the model predicts a certain class, it is correct 97.43% of the time on average across all classes. This suggests that the model has a low rate of false positives.\n",
        "•\tRecall: 0.974 (97.37%)\n",
        "o\tInterpretation: Recall is the ratio of correctly predicted positive observations to all observations in the actual class. A recall of 97.37% indicates that the model is able to correctly identify 97.37% of the actual positive cases across all classes, implying a low rate of false negatives.\n",
        "•\tF1-Score: 0.974 (97.37%)\n",
        "o\tInterpretation: The F1-score is the harmonic mean of precision and recall, providing a balanced measure of the model’s performance. With an F1-score of 97.37%, the model shows strong performance in terms of both precision and recall.\n",
        "2. Confusion Matrix Analysis\n",
        "•\tClass A:\n",
        "\tTrue Positives (TP): 25\n",
        "\tFalse Negatives (FN): 1\n",
        "\tFalse Positives (FP): 0\n",
        "\tInterpretation: Out of 26 instances of Class A, the model correctly identified 25. There was 1 instance where Class A was misclassified as Class H. The model never falsely predicted Class A when it was not actually A.\n",
        "•\tClass D:\n",
        "\tTrue Positives (TP): 19\n",
        "\tFalse Negatives (FN): 0\n",
        "\tFalse Positives (FP): 0\n",
        "\tInterpretation: The model perfectly identified all instances of Class D with no false negatives or false positives, indicating that the model is highly effective at recognizing this class.\n",
        "•\tClass H:\n",
        "\tTrue Positives (TP): 30\n",
        "\tFalse Negatives (FN): 1\n",
        "\tFalse Positives (FP): 1\n",
        "\tInterpretation: Out of 31 instances of Class H, the model correctly identified 30. There was 1 instance where Class H was misclassified as Class A, and 1 instance of Class A was incorrectly classified as Class H. This suggests a very slight confusion between Class A and Class H.\n",
        "3. Detailed Classification Report\n",
        "The classification report provides precision, recall, and F1-score for each individual class:\n",
        "•\tClass A:\n",
        "o\tPrecision: 1.00 (100%)\n",
        "\tThe model's predictions for Class A are completely accurate when it predicts Class A.\n",
        "o\tRecall: 0.96 (96%)\n",
        "\tThe model correctly identified 96% of all actual Class A instances.\n",
        "o\tF1-score: 0.98 (98%)\n",
        "\tThe overall performance for Class A is very high, with an excellent balance between precision and recall.\n",
        "•\tClass D:\n",
        "o\tPrecision: 0.95 (95%)\n",
        "\tWhen the model predicts Class D, it is correct 95% of the time.\n",
        "o\tRecall: 1.00 (100%)\n",
        "\tThe model identified all Class D instances correctly.\n",
        "o\tF1-score: 0.97 (97%)\n",
        "\tThis score shows the model has strong performance with very high precision and perfect recall.\n",
        "•\tClass H:\n",
        "o\tPrecision: 0.97 (97%)\n",
        "\tThe model’s predictions for Class H are correct 97% of the time.\n",
        "o\tRecall: 0.97 (97%)\n",
        "\tThe model correctly identified 97% of all actual Class H instances.\n",
        "o\tF1-score: 0.97 (97%)\n",
        "\tThis indicates consistently high performance in predicting Class H.\n",
        "4. Macro and Weighted Averages\n",
        "•\tMacro Average:\n",
        "o\tPrecision: 0.97\n",
        "o\tRecall: 0.98\n",
        "o\tF1-score: 0.97\n",
        "o\tInterpretation: The macro average is the unweighted mean of precision, recall, and F1-score across all classes. These values indicate the model performs uniformly well across all classes without giving preference to any particular class.\n",
        "•\tWeighted Average:\n",
        "o\tPrecision: 0.97\n",
        "o\tRecall: 0.97\n",
        "o\tF1-score: 0.97\n",
        "o\tInterpretation: The weighted average accounts for the support (the number of true instances for each class) when calculating the mean precision, recall, and F1-score. The high values here indicate that the model's overall performance is consistently strong, even when considering the class distribution.\n",
        "Conclusion\n",
        "The SVM model demonstrates excellent performance across all metrics. It achieves high accuracy, precision, recall, and F1-scores, indicating it is well-suited for the classification task at hand. The confusion matrix shows minimal misclassification, and the detailed classification report confirms that the model handles all classes effectively. The model's performance suggests it is a reliable tool for predicting the classes in the given dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "FKt5DkdZxjp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid,\n",
        "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV to the data\n",
        "grid_search.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Retrieve the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_svm_best = best_svm_model.predict(X_test_imputed)\n",
        "\n",
        "# Calculate the metrics for the best model\n",
        "accuracy_svm_best = accuracy_score(y_test, y_pred_svm_best)\n",
        "precision_svm_best = precision_score(y_test, y_pred_svm_best, average='weighted')\n",
        "recall_svm_best = recall_score(y_test, y_pred_svm_best, average='weighted')\n",
        "f1_svm_best = f1_score(y_test, y_pred_svm_best, average='weighted')\n",
        "conf_matrix_svm_best = confusion_matrix(y_test, y_pred_svm_best)\n",
        "class_report_svm_best = classification_report(y_test, y_pred_svm_best)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Print the results of the best model\n",
        "print(\"Best SVM Model Performance:\")\n",
        "print(\"Accuracy:\", accuracy_svm_best)\n",
        "print(\"Precision:\", precision_svm_best)\n",
        "print(\"Recall:\", recall_svm_best)\n",
        "print(\"F1-score:\", f1_svm_best)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_svm_best)\n",
        "print(\"\\nClassification Report:\\n\", class_report_svm_best)\n"
      ],
      "metadata": {
        "id": "TkGF2ojny9pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of SVM Model Performance\n",
        "After hyperparameter tuning, the SVM model achieved outstanding performance:\n",
        "\n",
        "Best Hyperparameters:\n",
        "\n",
        "C: 100, gamma: 0.001, kernel: 'rbf'\n",
        "These parameters were selected to maximize the model's accuracy and ability to generalize.\n",
        "Overall Metrics:\n",
        "\n",
        "Accuracy: 98.68%\n",
        "Precision: 98.73%\n",
        "Recall: 98.68%\n",
        "F1-Score: 98.68%\n",
        "These high metrics indicate the model performs exceptionally well, with very few misclassifications.\n",
        "Confusion Matrix:\n",
        "\n",
        "The model made only two minor errors, misclassifying one instance each between classes A and H, but perfectly identified all instances of Class D.\n",
        "Classification Report:\n",
        "\n",
        "Class A: Precision and recall are nearly perfect.\n",
        "Class D: Flawless performance with 100% precision, recall, and F1-score.\n",
        "Class H: Slightly lower precision but perfect recall, indicating strong overall performance.\n",
        "Macro and Weighted Averages:\n",
        "\n",
        "Both macro and weighted averages for precision, recall, and F1-score are extremely high at 99%, demonstrating consistent performance across all classes."
      ],
      "metadata": {
        "id": "7FuTYvpZ1Atq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lNVSpQcb-Ibz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}